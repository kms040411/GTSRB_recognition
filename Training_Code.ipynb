{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Training Code.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kms040411/GTSRB_recognition/blob/main/Training_Code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "233ZzuutUnAn"
      },
      "source": [
        "# Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "axBUyBF-UXmP"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt # this is for importing matplotlib.pyplot (library for graph plot)\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "from PIL import Image, ImageFilter, ImageEnhance, PpmImagePlugin\n",
        "import random"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gllvn4XuUsc-"
      },
      "source": [
        "# Mount Google Drive\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LQF4DfCTUr8Q",
        "outputId": "a18518ef-2dd0-4d22-d41b-c9e00a828387"
      },
      "source": [
        "from google.colab import files\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kLZl-_QRkA4w"
      },
      "source": [
        "# Modify Dataset\n",
        "We apply blurring, gaussian blurring, decolorization, darkening, exposure, adding noise to random image.\n",
        "\n",
        "## Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "19BzUilEkqPF"
      },
      "source": [
        "#path = \"/content/drive/My Drive/CS470/Project/GTSRB_Final_Training_Images/GTSRB/Final_Training/Images/\"\n",
        "label_num = 43\n",
        "#test_path = \"/content/drive/My Drive/CS470/Project/GTSRB_Final_Test_Images/GTSRB/Final_Test/Images/\""
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HHJlbOk_kXn1"
      },
      "source": [
        "## Functions\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EMdma-WJkcSK"
      },
      "source": [
        "def do_blur(img):\n",
        "    blurred = img.filter(ImageFilter.BoxBlur(2))\n",
        "    return blurred\n",
        "\n",
        "def do_gaussianblur(img):\n",
        "    blurred = img.filter(ImageFilter.GaussianBlur(2))\n",
        "    return blurred\n",
        "\n",
        "def do_decolorization(img):\n",
        "    grayscaled = img.convert(\"LA\")\n",
        "    grayscaled = grayscaled.convert(\"RGB\")\n",
        "    return grayscaled\n",
        "\n",
        "def do_darkening(img):\n",
        "    enhancer = ImageEnhance.Brightness(img)\n",
        "    darkened = enhancer.enhance(0.3)\n",
        "    return darkened\n",
        "\n",
        "def do_exposure(img):\n",
        "    enhancer = ImageEnhance.Contrast(img)\n",
        "    contrasted = enhancer.enhance(3.0)\n",
        "    enhancer2 = ImageEnhance.Brightness(contrasted)\n",
        "    exposured = enhancer.enhance(3.0)\n",
        "    return exposured\n",
        "\n",
        "def do_noise(img):\n",
        "    noisy = img.effect_spread(2)\n",
        "    return noisy"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "66tnruIWU_u-"
      },
      "source": [
        "# Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eeRuKFfLVBis"
      },
      "source": [
        "iter_num = 100\n",
        "learning_rate = 0.00005\n",
        "batch_size = 16\n",
        "\n",
        "Use_CNN = True\n",
        "Use_FRCNN = False\n",
        "\n",
        "is_continue = True\n",
        "model_path = \"/content/drive/My Drive/CS470/Project/pretrained.pth\"\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gypHf9tFVw8p"
      },
      "source": [
        "# Load Dataset\n",
        "\n",
        "## Local data version"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pnsBH0dhVxP8"
      },
      "source": [
        "class GSTRB_dataset():\n",
        "  def __init__(self, root):\n",
        "    self.root = root  # path for image root folder\n",
        "    self.basic_transform = data_transforms = transforms.Compose([\n",
        "                                                                  transforms.Resize((32, 32)),\n",
        "                                                                  transforms.ToTensor(),\n",
        "                                                                  transforms.Normalize((0, 0, 0), (1, 1, 1)) # Normalize to mean=0, std=1\n",
        "                                                                ])\n",
        "\n",
        "    # Read csv\n",
        "    self.csv_list = [list() for i in range(label_num)]\n",
        "    for i in range(0, label_num):\n",
        "      label_str = None\n",
        "      if i < 10:\n",
        "        label_str = \"0000\" + str(i)\n",
        "      else:\n",
        "        label_str = \"000\" + str(i)\n",
        "        \n",
        "      folder_path = self.root + label_str\n",
        "      with open(folder_path + \"/GT-\" + label_str + \".csv\") as f:\n",
        "        line = f.readline() # Ignore First line\n",
        "        while(True):\n",
        "            line = f.readline()\n",
        "            if line == \"\":\n",
        "                break\n",
        "            splited = line.split(\";\")\n",
        "            file_name = splited[0]\n",
        "            self.csv_list[i].append(file_name)\n",
        "    \n",
        "    # Split train data and validate data\n",
        "    self.train_list = [None for i in range(label_num)]\n",
        "    self.validate_list = [None for i in range(label_num)]\n",
        "    for i in range(0, label_num):\n",
        "      total_num = len(self.csv_list[i])\n",
        "      random.shuffle(self.csv_list[i])\n",
        "      split_index = int(total_num * 0.7)\n",
        "      self.train_list[i] = self.csv_list[i][:split_index]\n",
        "      self.validate_list[i] = self.csv_list[i][split_index:]\n",
        "\n",
        "  # Select random image and its index\n",
        "  def sample_one(self, data_set):\n",
        "    # Select label first\n",
        "    label = random.randrange(0, label_num)\n",
        "    label_str = None\n",
        "    if label < 10:\n",
        "      label_str = \"0000\" + str(label)\n",
        "    else:\n",
        "      label_str = \"000\" + str(label)\n",
        "\n",
        "    folder_path = self.root + label_str\n",
        "\n",
        "    # Sample image\n",
        "    # It is possible to select the same data, but the probability is very low.\n",
        "    target = random.sample(data_set[label], 1)[0]\n",
        "    img = Image.open(folder_path + \"/\" + target)\n",
        "\n",
        "    # Apply transform\n",
        "    transforms = [do_blur, do_gaussianblur, do_decolorization, do_darkening, do_exposure, do_noise, None]\n",
        "    rand_transform = random.sample(transforms, 1)[0]\n",
        "    if rand_transform is not None:\n",
        "      img = rand_transform(img)\n",
        "\n",
        "    # Basic transfrom\n",
        "    img = self.basic_transform(img)\n",
        "    img = torch.unsqueeze(img, 0)\n",
        "    label = torch.LongTensor([label])\n",
        "\n",
        "    return (img, label)\n",
        "  \n",
        "  def sample_train(self):\n",
        "    return self.sample_one(self.train_list)\n",
        "  \n",
        "  def sample_validate(self):\n",
        "    return self.sample_one(self.validate_list)"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yLQu6cCFRMeN"
      },
      "source": [
        "class GSTRB_test_dataset():\n",
        "  def __init__ (self, root):\n",
        "    self.root = root  # path for image root folder\n",
        "    self.basic_transform = data_transforms = transforms.Compose([\n",
        "                                                                  transforms.Resize((32, 32)),\n",
        "                                                                  transforms.ToTensor(),\n",
        "                                                                  transforms.Normalize((0, 0, 0), (1, 1, 1)) # Normalize to mean=0, std=1\n",
        "                                                                ])\n",
        "    # Read csv\n",
        "    self.train_list = list()\n",
        "    with open(folder_path + \"/GT-final_test.GT.csv\") as f:\n",
        "        line = f.readline() # Ignore First line\n",
        "        while(True):\n",
        "            line = f.readline()\n",
        "            if line == \"\":\n",
        "                break\n",
        "            splited = line.split(\";\")\n",
        "            file_name = splited[0]\n",
        "            label = splited[7]\n",
        "            self.csv_list[i].append((file_name, label))\n",
        "  \n",
        "  def sample_test(self):\n",
        "    target = random.sample(self.train_list, 1)[0]\n",
        "    img, label = Image.open(self.root + \"/\" + target)\n",
        "    label = int(label)\n",
        "\n",
        "    # Apply transform\n",
        "    transforms = [do_blur, do_gaussianblur, do_decolorization, do_darkening, do_exposure, do_noise, None]\n",
        "    #rand_transform = random.sample(transforms, 1)[0]\n",
        "    #if rand_transform is not None:\n",
        "    #  img = rand_transform(img)\n",
        "    \n",
        "    # Basic transfrom\n",
        "    img = self.basic_transform(img)\n",
        "    img = torch.unsqueeze(img, 0)\n",
        "    label = torch.LongTensor([label])\n",
        "\n",
        "    return (img, label)"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rf8wM6EhDYiw"
      },
      "source": [
        "## Kaggle version"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": "OK"
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 41
        },
        "id": "ZSdfoc8KCDWl",
        "outputId": "db95c1ed-ab40-49a2-f4b9-2e7e845899be"
      },
      "source": [
        "# We need to upload kaggle.json\n",
        "\n",
        "from google.colab import files\n",
        "files.upload()\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "# Download dataset from kaggle\n",
        "#!kaggle datasets download -d meowmeowmeowmeowmeow/gtsrb-german-traffic-sign --force\n",
        "#!unzip -q *.zip"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-3328dad9-c02f-4420-a043-326b911ac68e\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-3328dad9-c02f-4420-a043-326b911ac68e\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3mK4ISvZDftG"
      },
      "source": [
        "class GSTRB_train_dataset(torch.utils.data.Dataset):\n",
        "  def __init__(self, root, csv):\n",
        "    self.root = root\n",
        "    self.csv = pd.read_csv(csv)\n",
        "    self.basic_transform = data_transforms = transforms.Compose([\n",
        "                                                              transforms.Resize((32, 32)),\n",
        "                                                              transforms.ToTensor(),\n",
        "                                                              transforms.Normalize((0.3337, 0.3064, 0.3171), ( 0.2672, 0.2564, 0.2629))\n",
        "                                                            ])\n",
        "    self.counter = 0\n",
        "    self.transform_num = 7\n",
        "    \n",
        "  def __len__(self):\n",
        "    ans = len(self.csv) * self.transform_num\n",
        "    return ans\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    image = Image.open(os.path.join(self.root, str(self.csv.iloc[idx // self.transform_num, 7])))\n",
        "    label = torch.tensor(self.csv.iloc[idx // self.transform_num, 6]).long()\n",
        "        \n",
        "    # Apply transform\n",
        "    transforms = [do_blur, do_gaussianblur, do_decolorization, do_darkening, do_exposure, do_noise, None]\n",
        "    rand_transform = random.sample(transforms, 1)[0]\n",
        "    rand_transform = transforms[self.counter]\n",
        "    self.counter = self.counter + 1\n",
        "    self.counter = self.counter % self.transform_num\n",
        "    if rand_transform is not None:\n",
        "      image = rand_transform(image)\n",
        "    image = self.basic_transform(image)\n",
        "\n",
        "    pack = (image, label)\n",
        "    return pack\n",
        "\n",
        "train_dataset, valid_dataset = None, None\n",
        "if Use_CNN:\n",
        "  dataset_loader = GSTRB_train_dataset(\"/content/\", \"/content/Train.csv\")\n",
        "\n",
        "  # Split Dataset\n",
        "  train_Num = int(0.8*len(dataset_loader))\n",
        "  valid_Num = len(dataset_loader) - train_Num\n",
        "  train_dataset, valid_dataset = torch.utils.data.random_split(dataset_loader, [train_Num, valid_Num])\n",
        "\n",
        "  train_dataset = torch.utils.data.DataLoader(train_dataset,\n",
        "                                          batch_size=batch_size,\n",
        "                                          shuffle=True,\n",
        "                                          drop_last=True\n",
        "                                          )\n",
        "  valid_dataset = torch.utils.data.DataLoader(valid_dataset,\n",
        "                                          batch_size=batch_size,\n",
        "                                          shuffle=False,\n",
        "                                          drop_last=True\n",
        "                                          )"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gqs4PX_h5bKT"
      },
      "source": [
        "## Dataset for Fast RCNN Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Cp1W21B32OX"
      },
      "source": [
        "class GTSRB_frcnn_train_dataset(torch.utils.data.Dataset):\n",
        "  def __init__(self, root, csv):\n",
        "    self.root = root\n",
        "    self.csv = pd.read_csv(csv)\n",
        "    self.basic_transform = data_transforms = transforms.Compose([\n",
        "                                                              transforms.Resize((32, 32)),\n",
        "                                                              transforms.ToTensor(),\n",
        "                                                              transforms.Normalize((0.3337, 0.3064, 0.3171), ( 0.2672, 0.2564, 0.2629))\n",
        "                                                            ])\n",
        "    self.counter = 0\n",
        "    self.transform_num = 7\n",
        "  \n",
        "  def __len__(self):\n",
        "    ans = len(self.csv) * self.transform_num\n",
        "    return ans\n",
        "  \n",
        "  def __getitem__(self, idx):\n",
        "    image = Image.open(os.path.join(self.root, str(self.csv.iloc[idx // self.transform_num, 7])))\n",
        "    label = torch.tensor([self.csv.iloc[idx // self.transform_num, 6]]).long()\n",
        "        \n",
        "    # Apply transform\n",
        "    transforms = [do_blur, do_gaussianblur, do_decolorization, do_darkening, do_exposure, do_noise, None]\n",
        "    rand_transform = random.sample(transforms, 1)[0]\n",
        "    rand_transform = transforms[self.counter]\n",
        "    self.counter = self.counter + 1\n",
        "    self.counter = self.counter % self.transform_num\n",
        "    if rand_transform is not None:\n",
        "      image = rand_transform(image)\n",
        "    image = self.basic_transform(image)\n",
        "\n",
        "    # Get boxes\n",
        "    box = torch.tensor([[self.csv.iloc[idx // self.transform_num, 2], self.csv.iloc[idx // self.transform_num, 3], self.csv.iloc[idx // self.transform_num, 4], self.csv.iloc[idx // self.transform_num, 5]]])\n",
        "\n",
        "    pack = (image, label, box)\n",
        "    return pack\n",
        "\n",
        "train_frcnn_dataset, valid_frcnn_dataset = None, None\n",
        "if Use_FRCNN:\n",
        "  frcnn_dataset_loader = GTSRB_frcnn_train_dataset(\"/content/\", \"/content/Train.csv\")\n",
        "\n",
        "  # Split Dataset\n",
        "  train_Num = int(0.8*len(frcnn_dataset_loader))\n",
        "  valid_Num = len(frcnn_dataset_loader) - train_Num\n",
        "  train_frcnn_dataset, valid_frcnn_dataset = torch.utils.data.random_split(frcnn_dataset_loader, [train_Num, valid_Num])\n",
        "\n",
        "  train_frcnn_dataset = torch.utils.data.DataLoader(train_frcnn_dataset,\n",
        "                                          batch_size=batch_size,\n",
        "                                          shuffle=True,\n",
        "                                          drop_last=True\n",
        "                                          )\n",
        "  valid_frcnn_dataset = torch.utils.data.DataLoader(valid_frcnn_dataset,\n",
        "                                          batch_size=batch_size,\n",
        "                                          shuffle=False,\n",
        "                                          drop_last=True\n",
        "                                          )"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xy4Q6xWMV2L_"
      },
      "source": [
        "# Define Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SvgHADU83OR6"
      },
      "source": [
        "'''class SimpleCNN(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(SimpleCNN, self).__init__()\n",
        "    # Spatial Transformation Network\n",
        "    self.localization = nn.Sequential(\n",
        "                                        nn.Conv2d(3, 8, kernel_size=7),\n",
        "                                        nn.MaxPool2d(2, stride=2),\n",
        "                                        nn.ReLU(True),\n",
        "                                        nn.Conv2d(8, 10, kernel_size=5),\n",
        "                                        nn.MaxPool2d(2, stride=2),\n",
        "                                        nn.ReLU(True)\n",
        "                                    )\n",
        "    \n",
        "    # Regressor for the affine matrix\n",
        "    self.fc_loc = nn.Sequential(\n",
        "                                  nn.Linear(10 * 4 * 4, 32),\n",
        "                                  nn.ReLU(True),\n",
        "                                  nn.Linear(32, 3 * 2)\n",
        "                              )\n",
        "\n",
        "    # CNN\n",
        "    self.conv1 = nn.Conv2d(in_channels=3, out_channels=100, kernel_size=5)\n",
        "    self.relu1 = nn.ReLU()\n",
        "    self.batchnorm1 = nn.BatchNorm2d(100)\n",
        "    self.maxpool1 = nn.MaxPool2d(2)\n",
        "    self.drop1 = nn.Dropout2d()\n",
        "    self.conv2 = nn.Conv2d(in_channels=100, out_channels=150, kernel_size=3)\n",
        "    self.relu2 = nn.ReLU()\n",
        "    self.batchnorm2 = nn.BatchNorm2d(150)\n",
        "    self.maxpool2 = nn.MaxPool2d(2)\n",
        "    self.drop2 = nn.Dropout2d()\n",
        "    self.conv3 = nn.Conv2d(in_channels=150, out_channels=250, kernel_size=3)\n",
        "    self.relu3 = nn.ReLU()\n",
        "    self.batchnorm3 = nn.BatchNorm2d(250)\n",
        "    self.maxpool3 = nn.MaxPool2d(2)\n",
        "    self.drop3 = nn.Dropout2d()\n",
        "    \n",
        "\n",
        "    # Full-Connected\n",
        "    self.fc1 = nn.Linear(250 * 2 * 2, 350)\n",
        "    self.relu4 = nn.ReLU()\n",
        "    self.fc2 = nn.Linear(350, label_num)\n",
        "    self.relu5 = nn.ReLU()\n",
        "\n",
        "  def forward(self, x):\n",
        "    output = x\n",
        "\n",
        "    # Spatial Transformation Network\n",
        "    output = self.localization(output)\n",
        "    output = output.view(-1, 10 * 4 * 4)\n",
        "    theta = self.fc_loc(output)\n",
        "    theta = theta.view(-1, 2, 3)\n",
        "    grid = torch.nn.functional.affine_grid(theta, x.size(), align_corners=False)\n",
        "    output = torch.nn.functional.grid_sample(x, grid, align_corners=False)\n",
        "\n",
        "    # CNN\n",
        "    output = self.conv1(output)\n",
        "    output = self.relu1(output)\n",
        "    output = self.maxpool1(output)\n",
        "    output = self.batchnorm1(output)\n",
        "    output = self.drop1(output)\n",
        "\n",
        "    output = self.conv2(output)\n",
        "    output = self.relu2(output)\n",
        "    output = self.maxpool2(output)\n",
        "    output = self.batchnorm2(output)\n",
        "    output = self.drop2(output)\n",
        "\n",
        "    output = self.conv3(output)\n",
        "    output = self.relu3(output)\n",
        "    output = self.maxpool3(output)\n",
        "    output = self.batchnorm3(output)\n",
        "    output = self.drop3(output)\n",
        "\n",
        "    # Full-Connected\n",
        "    output = output.view(-1, 250 * 2 * 2)\n",
        "    output = self.fc1(output)\n",
        "    output = self.relu4(output)\n",
        "    output = self.fc2(output)\n",
        "    output = self.relu5(output)\n",
        "    output = nn.functional.log_softmax(output, dim=1)\n",
        "    return output\n",
        "'''\n",
        "import torch.nn.functional as F\n",
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "        \n",
        "        # CNN layers\n",
        "        self.conv1 = nn.Conv2d(3, 100, kernel_size=5)\n",
        "        self.bn1 = nn.BatchNorm2d(100)\n",
        "        self.conv2 = nn.Conv2d(100, 150, kernel_size=3)\n",
        "        self.bn2 = nn.BatchNorm2d(150)\n",
        "        self.conv3 = nn.Conv2d(150, 250, kernel_size=3)\n",
        "        self.bn3 = nn.BatchNorm2d(250)\n",
        "        self.conv_drop = nn.Dropout2d()\n",
        "        self.fc1 = nn.Linear(250*2*2, 350)\n",
        "        self.fc2 = nn.Linear(350, label_num)\n",
        "\n",
        "        self.localization = nn.Sequential(\n",
        "            nn.Conv2d(3, 8, kernel_size=7),\n",
        "            nn.MaxPool2d(2, stride=2),\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv2d(8, 10, kernel_size=5),\n",
        "            nn.MaxPool2d(2, stride=2),\n",
        "            nn.ReLU(True)\n",
        "            )\n",
        "\n",
        "        # Regressor for the 3 * 2 affine matrix\n",
        "        self.fc_loc = nn.Sequential(\n",
        "            nn.Linear(10 * 4 * 4, 32),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(32, 3 * 2)\n",
        "            )\n",
        "   \n",
        "        # Initialize the weights/bias with identity transformation\n",
        "        self.fc_loc[2].weight.data.zero_()\n",
        "        self.fc_loc[2].bias.data.copy_(torch.tensor([1, 0, 0, 0, 1, 0], dtype=torch.float))\n",
        "\n",
        "\n",
        "    # Spatial transformer network forward function\n",
        "    def stn(self, x):\n",
        "        xs = self.localization(x)\n",
        "        xs = xs.view(-1, 10 * 4 * 4)\n",
        "        theta = self.fc_loc(xs)\n",
        "        theta = theta.view(-1, 2, 3)\n",
        "        grid = F.affine_grid(theta, x.size())\n",
        "        x = F.grid_sample(x, grid)\n",
        "        return x\n",
        "\n",
        "    def forward(self, x):\n",
        "        # transform the input\n",
        "        x = self.stn(x)\n",
        "\n",
        "        # Perform forward pass\n",
        "        x = self.bn1(F.max_pool2d(F.leaky_relu(self.conv1(x)),2))\n",
        "        x = self.conv_drop(x)\n",
        "        x = self.bn2(F.max_pool2d(F.leaky_relu(self.conv2(x)),2))\n",
        "        x = self.conv_drop(x)\n",
        "        x = self.bn3(F.max_pool2d(F.leaky_relu(self.conv3(x)),2))\n",
        "        x = self.conv_drop(x)\n",
        "        x = x.view(-1, 250*2*2)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.dropout(x, training=self.training)\n",
        "        x = self.fc2(x)\n",
        "        return F.log_softmax(x, dim=1)"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KgIaurJnV2iL"
      },
      "source": [
        "def weights_init(m):\n",
        "  if isinstance(m, nn.Linear) or isinstance(m, nn.Conv2d):\n",
        "    nn.init.normal_(m.weight, mean=0.5, std=1.0)\n",
        "    nn.init.normal_(m.bias, mean=0.5, std=1.0)\n",
        "\n",
        "model = None\n",
        "if Use_CNN:\n",
        "  model = SimpleCNN()\n",
        "  model = model.to(device)"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bkEtg57447iI"
      },
      "source": [
        "import torchvision\n",
        "\n",
        "frcnn_model = None\n",
        "if Use_FRCNN:\n",
        "  frcnn_model = torchvision.models.detection.fasterrcnn_resnet50_fpn(num_classes=label_num)\n",
        "  frcnn_model = frcnn_model.to(device)"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zZgvwBuIUZNj"
      },
      "source": [
        "# Training Code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FqUkxVxg0Hns"
      },
      "source": [
        "def label(prediction):\n",
        "  _, pred_label = torch.max(prediction, 1)\n",
        "  return pred_label\n",
        "\n",
        "def save_checkpoint(model, optimizer, epoch, model_path):\n",
        "  state = {\n",
        "      'state_dict': model,\n",
        "      'optimizer': optimizer,\n",
        "      'epoch': epoch\n",
        "  }"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tam5GFOoUzTx",
        "outputId": "4c2cf8ae-9747-4d5f-c022-4fb1ba979ff1"
      },
      "source": [
        "if Use_CNN:\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "  criterion = torch.nn.CrossEntropyLoss()\n",
        "  start_epoch = 1\n",
        "  if is_continue:\n",
        "    print(\"Load previous model parameters\")\n",
        "    loaded = torch.load(model_path)\n",
        "    model.load_state_dict(loaded)\n",
        "    #optimizer.load_state_dict(loaded['optimizer'])\n",
        "    #start_epoch = loaded['epoch'] + 1\n",
        "  else:\n",
        "    model.apply(weights_init)\n",
        "    # Initalize STN\n",
        "    model.fc_loc[2].weight.data.zero_()\n",
        "    model.fc_loc[2].bias.data = torch.tensor([1, 0, 0, 0, 1, 0], dtype=torch.float).cuda()\n",
        "\n",
        "  # For each epoch, multiply 0.95 to learning rate\n",
        "  scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma= 0.99)\n",
        "\n",
        "  # Statistics for total training\n",
        "  train_loss = np.zeros(iter_num, dtype=float)\n",
        "  validation_loss = np.zeros(iter_num, dtype=float)\n",
        "  train_accuracy = np.zeros(iter_num, dtype=float)\n",
        "  validation_accuracy = np.zeros(iter_num, dtype=float)\n",
        "\n",
        "  previous_best_accurary = None\n",
        "\n",
        "  for iter in range(start_epoch, iter_num + start_epoch):\n",
        "    # Set training mode\n",
        "    model.train()\n",
        "\n",
        "    # Statistics for current iteration\n",
        "    current_loss = 0.0\n",
        "    total_prediction = 0.0\n",
        "    correct_prediction = 0.0\n",
        "    i = 0\n",
        "    for _, (x, answer) in enumerate(train_dataset):\n",
        "      print(\"\\r\", end=\"\")\n",
        "      i = i + 1\n",
        "      print(\"train_dataset: \",i, \"/\", len(train_dataset), end=\"\")\n",
        "      x, answer = x.cuda(), answer.cuda()\n",
        "      optimizer.zero_grad()\n",
        "      prediction = model(x)\n",
        "      loss = criterion(prediction, answer)  # We use cross entropy loss for loss calculation\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      current_loss += loss.item()\n",
        "      total_prediction += x.data.size(0)\n",
        "      correct_prediction += (label(prediction) == answer.data).sum().item()\n",
        "    \n",
        "    accuracy = correct_prediction * 100.0 / total_prediction\n",
        "    train_loss[iter - start_epoch] = current_loss / total_prediction\n",
        "    train_accuracy[iter - start_epoch] = accuracy\n",
        "\n",
        "    scheduler.step() \n",
        "\n",
        "    if iter % 1 == 0:\n",
        "      print(str(iter) + \"/\" + str(iter_num + start_epoch) + \": train loss - \" + str(train_loss[iter - start_epoch]) + \", train accuracy - \" + str(train_accuracy[iter - start_epoch]) + \"%, lr: \" + str(scheduler.get_last_lr()))\n",
        "\n",
        "    # Set validation mode\n",
        "    model.eval()\n",
        "\n",
        "    # Statistics for current iteration\n",
        "    current_loss = 0.0\n",
        "    total_prediction = 0.0\n",
        "    correct_prediction = 0.0\n",
        "    #for batch_num in range(batch_size):\n",
        "    #  x, answer = dataset.sample_validate()\n",
        "    with torch.no_grad():\n",
        "      i=0\n",
        "      for _, (x, answer) in enumerate(valid_dataset):\n",
        "        print(\"\\r\", end=\"\")\n",
        "        i = i + 1\n",
        "        print(\"valid_dataset: \",i, \"/\", len(valid_dataset), end=\"\")\n",
        "        x, answer = x.cuda(), answer.cuda()\n",
        "        prediction = model(x)\n",
        "        loss = criterion(prediction, answer)  # We use cross entropy loss for loss calculation\n",
        "\n",
        "        current_loss += loss.item()\n",
        "        total_prediction += x.data.size(0)\n",
        "        correct_prediction += (label(prediction) ==  answer.data).sum().item()\n",
        "      \n",
        "      accuracy = correct_prediction * 100.0 / total_prediction\n",
        "      train_loss[iter - start_epoch] = current_loss / total_prediction\n",
        "      train_accuracy[iter - start_epoch] = accuracy\n",
        "\n",
        "    if iter % 1 == 0:\n",
        "      print(str(iter) + \"/\" + str(iter_num + start_epoch) + \": validation loss - \" + str(train_loss[iter - start_epoch]) + \", validation accuracy - \" + str(train_accuracy[iter - start_epoch]) + \"%\")\n",
        "    \n",
        "    # Save checkpoint\n",
        "    if iter % 50 == 0:\n",
        "      print(\"Save Current Model Parameter\")\n",
        "      save_checkpoint(model.state_dict(), optimizer.state_dict(), iter, model_path)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Load previous model parameters\n",
            "\rtrain_dataset:  1 / 13723\rtrain_dataset:  2 / 13723\rtrain_dataset:  3 / 13723\rtrain_dataset:  4 / 13723\rtrain_dataset:  5 / 13723\rtrain_dataset:  6 / 13723\rtrain_dataset:  7 / 13723"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:3448: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.\n",
            "  warnings.warn(\"Default grid_sample and affine_grid behavior has changed \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:3385: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.\n",
            "  warnings.warn(\"Default grid_sample and affine_grid behavior has changed \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "train_dataset:  6280 / 13723"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gFB3Ov9558yg"
      },
      "source": [
        "## Train Fast RCNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vUZTjbK657z-"
      },
      "source": [
        "if Use_FRCNN:\n",
        "  optimizer = torch.optim.Adam(frcnn_model.parameters(), lr=learning_rate)\n",
        "  start_epoch = 1\n",
        "  if is_continue:\n",
        "    print(\"Load previous model parameters\")\n",
        "    loaded = torch.load(model_path)\n",
        "    model.load_state_dict(loaded['state_dict'])\n",
        "    optimizer.load_state_dict(loaded['optimizer'])\n",
        "    start_epoch = loaded['epoch'] + 1\n",
        "\n",
        "  # For each epoch, multiply 0.95 to learning rate\n",
        "  scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma= 0.99)\n",
        "\n",
        "  # Statistics for total training\n",
        "  train_loss = np.zeros(iter_num, dtype=float)\n",
        "  validation_loss = np.zeros(iter_num, dtype=float)\n",
        "  train_accuracy = np.zeros(iter_num, dtype=float)\n",
        "  validation_accuracy = np.zeros(iter_num, dtype=float)\n",
        "\n",
        "  previous_best_accurary = None\n",
        "\n",
        "  for iter in range(start_epoch, iter_num + start_epoch):\n",
        "    # Set training mode\n",
        "    frcnn_model.train()\n",
        "\n",
        "    # Statistics for current iteration\n",
        "    current_loss = 0.0\n",
        "    total_prediction = 0.0\n",
        "    correct_prediction = 0.0\n",
        "    i = 0\n",
        "    for _, (x, label, box) in enumerate(train_frcnn_dataset):\n",
        "      print(\"\\r\", end=\"\")\n",
        "      i = i + 1\n",
        "      print(i, \"/\", len(train_frcnn_dataset), end=\"\")\n",
        "      x, label, box = x.to(device), label.to(device), box.to(device)\n",
        "      images = list(image for image in x)\n",
        "      targets = []\n",
        "      for i in range(len(images)):\n",
        "        d = {}\n",
        "        d['boxes'] = box[i]\n",
        "        d['labels'] = label[i]\n",
        "        targets.append(d)\n",
        "\n",
        "      prediction = frcnn_model(images, targets)\n",
        "      optimizer.zero_grad()\n",
        "      loss = prediction['loss_classifier'].mean() + prediction['loss_classifier'].mean() + prediction['loss_objectness'].mean() + prediction['loss_rpn_box_reg'].mean()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      current_loss += loss.item()\n",
        "      total_prediction += x.data.size(0)\n",
        "      #correct_prediction += (label( == answer.data).sum().item()\n",
        "    \n",
        "    accuracy = correct_prediction * 100.0 / total_prediction\n",
        "    train_loss[iter - start_epoch] = current_loss / total_prediction\n",
        "    train_accuracy[iter - start_epoch] = accuracy\n",
        "\n",
        "    scheduler.step() \n",
        "    \n",
        "    if iter % 1 == 0:\n",
        "      print(str(iter) + \"/\" + str(iter_num + start_epoch) + \": train loss - \" + str(train_loss[iter - start_epoch]) + \", train accuracy - \" + str(train_accuracy[iter - start_epoch]) + \"%, lr: \" + str(scheduler.get_last_lr()))\n",
        "\n",
        "    # Set validation mode\n",
        "    frcnn_model.eval()\n",
        "\n",
        "    # Statistics for current iteration\n",
        "    current_loss = 0.0\n",
        "    total_prediction = 0.0\n",
        "    correct_prediction = 0.0\n",
        "    #for batch_num in range(batch_size):\n",
        "    #  x, answer = dataset.sample_validate()\n",
        "    with torch.no_grad():\n",
        "      for _, (x, label, box) in enumerate(valid_frcnn_dataset):\n",
        "        x, label, box = x.to(device), label.to(device), box.to(device)\n",
        "        images = list(image for image in x)\n",
        "        targets = []\n",
        "        for i in range(len(images)):\n",
        "          d = {}\n",
        "          d['boxes'] = box[i]\n",
        "          d['labels'] = label[i]\n",
        "          targets.append(d)\n",
        "\n",
        "        prediction = frcnn_model(images, targets)\n",
        "        loss = prediction['loss_classifier'].mean() + prediction['loss_classifier'].mean() + prediction['loss_objectness'].mean() + prediction['loss_rpn_box_reg'].mean()\n",
        "\n",
        "        current_loss += loss.item()\n",
        "        total_prediction += x.data.size(0)\n",
        "        #correct_prediction += (label(prediction) ==  answer.data).sum().item()\n",
        "      \n",
        "      accuracy = correct_prediction * 100.0 / total_prediction\n",
        "      train_loss[iter - start_epoch] = current_loss / total_prediction\n",
        "      train_accuracy[iter - start_epoch] = accuracy\n",
        "\n",
        "    if iter % 1 == 0:\n",
        "      print(str(iter) + \"/\" + str(iter_num + start_epoch) + \": validation loss - \" + str(train_loss[iter - start_epoch]) + \", validation accuracy - \" + str(train_accuracy[iter - start_epoch]) + \"%\")\n",
        "    \n",
        "    # Save checkpoint\n",
        "    if iter % 50 == 0:\n",
        "      print(\"Save Current Model Parameter\")\n",
        "      save_checkpoint(frcnn_model.state_dict(), optimizer.state_dict(), iter, model_path)\n",
        "      "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}